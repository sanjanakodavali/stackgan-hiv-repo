# StackGAN-HIV — Text→Figure Synthesis for HIV Entry & Drug-Target Structures

**Goal.** Keep the original [StackGAN](https://github.com/hanzhanggit/StackGAN) recipe (two-stage GAN + conditioning augmentation) and generate publication-style molecular renderings from short HIV captions (CCR5/CXCR4, Env–CD4, RT/PR/IN complexes).

**Why this matters.** Clean, public-domain structural data from the Protein Data Bank (PDB) lets us train a caption-conditioned image generator without privacy hurdles. Outputs are research-lab-style visuals (surface/cartoon/ligand views) that we can evaluate with **FID/KID** and **text–image retrieval (Recall@K)** using a biomedical CLIP text tower.

---

## Repository Layout

```
stackgan-hiv/
├─ data/                 # image/caption pairs + artifacts generated by scripts
│  ├─ images/            # rendered PNGs (will be added in later milestone)
│  ├─ pdb_meta.jsonl     # placeholder metadata (script-generated)
│  └─ pairs.csv          # id,pdb_id,caption,image_path,view (script-generated)
├─ docs/                 # blueprint PDF, diagrams, wireframes
│  ├─ architecture.png
│  ├─ wireframe.png
│  └─ blueprint.pdf
├─ notebooks/
│  └─ setup.ipynb        # environment check + sample plot + data I/O demo
├─ results/              # exploratory visuals / early outputs
│  └─ placeholder.png
├─ scripts/              # dataset building utilities
│  ├─ pdb_fetch_render.py
│  └─ build_captions.py
├─ src/                  # training/eval helpers that wrap original StackGAN code
├─ ui/                   # placeholder for Streamlit/Gradio demo
├─ .github/workflows/
│  └─ ci.yml
├─ requirements.txt
├─ environment.yml
└─ README.md
```

---

## Installation

We recommend Python 3.10+ in a fresh environment.

**Option A — conda**
```bash
conda env create -f environment.yml
conda activate stackgan-hiv
```

**Option B — pip + venv**
```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip
pip install -r requirements.txt
```

Quick smoke test:
```bash
python - <<'PY'
import sys, torch, transformers
print("Python:", sys.version.split()[0])
print("Torch:", torch.__version__)
print("Transformers:", transformers.__version__)
PY
```

---

## Quick Start (Deliverable-1 proof)

1) **Generate tiny dataset artifacts (collection step).**
```bash
python scripts/pdb_fetch_render.py
python scripts/build_captions.py
```
This creates:
- `data/pdb_meta.jsonl`
- `data/pairs.csv` with columns: `id,pdb_id,caption,image_path,view`

2) **Run the setup notebook (visible output).**
- Interactive:
  ```bash
  jupyter notebook notebooks/setup.ipynb
  ```
- Or headless:
  ```bash
  jupyter nbconvert --to notebook --execute notebooks/setup.ipynb     --output notebooks/setup_executed.ipynb
  ```

3) **Show early EDA (one cell inside the notebook).**
```python
import pandas as pd
df = pd.read_csv("data/pairs.csv")
df.head(), len(df)
```

You’ll see a small table + count, and the notebook already renders a sample plot—this fulfills the “runs cleanly and produces visible output” requirement.

---

## Dataset Information

- **Core dataset (single source):** RCSB **Protein Data Bank (PDB)** HIV/host complexes.
  - We will **render** structures (e.g., CCR5/CXCR4, Env–CD4, Reverse Transcriptase, Protease, Integrase + ligands/antibodies) into PNGs at **64×64** and **256×256** across multiple views (surface/cartoon/ligand sticks).
  - We **auto-caption** each render from PDB metadata (short, templated captions).
  - **Format:** PNG images in `data/images/` and a `data/pairs.csv` mapping captions → image paths.
  - **Access & license:** Public domain (**CC0**); attribution to depositors is appreciated but not required.

- **Optional (later):** A small microscopy add-on from **Human Protein Atlas** for **CCR5/CXCR4** IF/IHC (license **CC BY-SA 4.0**, attribution required). *Not needed for Deliverable-1.*

---

## Planned Architecture (high level)

Data → **Text Encoder (BiomedCLIP)** → **Conditioning Augmentation (μ, Σ)** + noise → **StackGAN Stage-I (64×64)** → **Stage-II (256×256)** → UI

See `docs/architecture.png`.

- **Algorithms:** Original StackGAN with conditioning augmentation; two-stage training.
- **Encoders:** Biomedical CLIP-style text tower for captions (frozen/fine-tuned later).
- **Evaluation:** FID/KID; text–image retrieval **Recall@K** using the same text tower.
- **Interface:** Simple **Streamlit/Gradio** app to take a caption + seed and render a 2×2 grid, plus nearest real references for context (see `docs/wireframe.png`).

---

## Usage Notes (current milestone)

- For Deliverable-1, we verify environment + data pipeline I/O. Training scripts (Stage-I/II) will land in the next milestone under `src/` (wrapping the original StackGAN code path) with configs in `cfg/`.

- Reproducibility: we will expose `seed`, `#images`, and `stage` in the UI; every generated image will be tagged with the exact caption + seed.

---

## Responsible AI & Licensing

- **Synthetic images only**; watermark outputs as synthetic where appropriate.
- **Transparency:** show caption + seed and nearest real references in the demo.
- **Licensing:** PDB structures are CC0; any optional HPA images require attribution under CC BY-SA 4.0.
- **Compute:** use mixed precision, early stopping, and reasonable batch sizes.

---

## Assignment Tips (what graders will look for)

- The repo **runs** (notebook executes; sample plot visible).
- **Data step** demonstrated (scripts wrote `pairs.csv`; notebook loads it).
- Clear **architecture & UI** plan (see `docs/architecture.png`, `docs/wireframe.png`, and `docs/blueprint.pdf`).
- Repro steps are obvious (install, run notebook, see outputs).

---

## Author

- **Name:** Aslesha Sanjana Kodavali  
- **Email:** sanjanakodavali10@gmail.com
